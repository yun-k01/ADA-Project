{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yun-k01/ADA-Project/blob/main/ADA_7325_Assn2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj7emI9-AiAr"
      },
      "source": [
        "# CISC 351 Assignment 2\n",
        "## Yun Kyaw, 20177325"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bes2kpyLAC1D",
        "outputId": "eff0befb-ba40-466d-cabc-ba7b35ddaa59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  archive.zip\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Change directory to MyDrive\n",
        "import os\n",
        "import imageio\n",
        "import glob\n",
        "import random\n",
        "from collections import Counter\n",
        "from google.colab.patches import cv2_imshow\n",
        "os.chdir(\"/content/drive/MyDrive\")\n",
        "! unzip archive.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1npQL10nAqJt"
      },
      "source": [
        "### Image Classification using C-NN\n",
        "Creating a basic CNN model that can identify if an image is related to the following 13 subcategories: Topwear, Bottomwear, Innerwear, Bags, Watches, Jewellery, Eyewear, Wallets, Shoes, Sandal, Makeup, Fragrance, or Others. Specifically, the dataset contains 44,441 fashion product images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZicITe6L4wC"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3WKiyyieAZkk",
        "outputId": "84eb377d-9064-484e-9a6c-c9d846f8ea81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c268a70aa530>:3: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  pic = imageio.imread('./myntradataset/images/'+ random_pic_file)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=60x80 at 0x7F5796C64430>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        }
      ],
      "source": [
        "# looking at a random image\n",
        "random_pic_file = random.choice(os.listdir('./myntradataset/images/'))\n",
        "pic = imageio.imread('./myntradataset/images/'+ random_pic_file)\n",
        "cv2_imshow(pic)\n",
        "height, width, channels = pic.shape\n",
        "print(f'original height, width, and channels of each image: {height} {width} {channels}')\n",
        "\n",
        "# looking at label distribution\n",
        "file_path = [\"./myntradataset/images/\"]\n",
        "for path in file_path:\n",
        "  labels = []\n",
        "  files = glob.glob(path+\"*\")\n",
        "  file_count = len(files)\n",
        "  print(f'There are {file_count} files')\n",
        "\n",
        "  if \"test\" in path:\n",
        "    continue\n",
        "\n",
        "  labels = [int(filename[len(path):].split(\".\")[0]) for filename in files]  # here we make each label an integer, taking the the name X from X.jpg\n",
        "\n",
        "  counts = Counter(labels)\n",
        "  total_count = len(labels)\n",
        "  for value, count in sorted(counts.items(), key=lambda x: x[0]):\n",
        "    distribution = count / total_count\n",
        "    print(f'{value}: {count} ({distribution:.2%})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu5j3IPnBEzQ"
      },
      "source": [
        "Preparing the libraries and GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsD65RleBFRK"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
        "from torchvision.datasets import DatasetFolder, VisionDataset\n",
        "\n",
        "# This is for the progress bar.\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "myseed = 12345"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ijzhJ7RBHx8"
      },
      "outputs": [],
      "source": [
        "# Setting up PyTorch\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw37LbhVBJjc"
      },
      "source": [
        "Preparing datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tfm = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # here we resize the imag to h=w=128\n",
        "    transforms.ToTensor(),  # here we transform the image to tensor\n",
        "])\n",
        "\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "uCoQh9jdpvpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "KWqp8gfTAN2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionDataset(Dataset):\n",
        "    def __init__(self,path,tfm=test_tfm,files = None):\n",
        "        super(FashionDataset).__init__()\n",
        "        self.path = path\n",
        "        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n",
        "        if files != None:\n",
        "            self.files = files\n",
        "        print(f\"One {path} sample\",self.files[0])\n",
        "        self.transform = tfm\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "  \n",
        "    def __getitem__(self,idx):\n",
        "        fname = self.files[idx]\n",
        "        im = Image.open(fname)\n",
        "        im = self.transform(im)\n",
        "        print(fname)\n",
        "  \n",
        "        try:\n",
        "            label = int(fname.split(\"/\")[-1].split(\".\")[0])\n",
        "            label -= 1\n",
        "        except:\n",
        "            label = -1 # test has no label\n",
        "        return im,label"
      ],
      "metadata": {
        "id": "8WB7d5N-AQG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(labels))\n",
        "print(max(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9RvRTwilIKC",
        "outputId": "aa8dcd46-1638-43fd-9532-0182b6c63aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10150)\n",
            "tensor(17866)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHXw8NmoBQOP"
      },
      "source": [
        "#### Creating the first CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXudHG9YBN-m"
      },
      "outputs": [],
      "source": [
        "class FirstCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FirstCNN, self).__init__()\n",
        "       \n",
        "    # In the following sequence, we loop between creating a convolutional layer,\n",
        "    # and max pooling to then flatten the image and create fully connected\n",
        "    # feedforward network\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
        "            \n",
        "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512*4*4, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 11)\n",
        "        )\n",
        "   \n",
        "    def forward(self, x):\n",
        "        out = self.cnn(x)\n",
        "        out = out.view(out.size()[0], -1)\n",
        "        return self.fc(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i8JZf2MBVcb"
      },
      "source": [
        "Loading the Training and Testing datasets\n",
        "\n",
        "There was an error in uploading the files as there were commas in the productname column - these commas were manually removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n63LKd_BV8R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "02420ffc-e1c7-408f-9e18-3a35682db585"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-242eabe0-44dd-498d-8adc-1466a973cb02\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-242eabe0-44dd-498d-8adc-1466a973cb02\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.csv to test (8).csv\n",
            "Saving train.csv to train (8).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import io\n",
        "\n",
        "train_set = pd.read_csv(io.BytesIO(uploaded['train.csv']))\n",
        "test_set = pd.read_csv(io.BytesIO(uploaded['test.csv']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_exp_name = \"sample\"\n",
        "batch_size = 64\n",
        "_dataset_dir = \"./myntradataset\"\n",
        "\n",
        "# Construct datasets.\n",
        "# The argument \"loader\" tells how torchvision reads the data.\n",
        "\n",
        "train_set = FashionDataset(os.path.join(_dataset_dir, \"images\"), tfm=train_tfm)\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "valid_set = FashionDataset(os.path.join(_dataset_dir, \"images\"), tfm=test_tfm)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSsMMH_rX1MF",
        "outputId": "2967455a-1720-46b3-a25c-e583a022c524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One ./myntradataset/images sample ./myntradataset/images/10000.jpg\n",
            "One ./myntradataset/images sample ./myntradataset/images/10000.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"cuda\" only when GPUs are available.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# The number of training epochs and patience.\n",
        "n_epochs = 4\n",
        "patience = 300 # If no improvement in 'patience' epochs, early stop\n",
        "\n",
        "# Initialize a model, and put it on the device specified.\n",
        "model = FirstCNN().to(device)\n",
        "\n",
        "# For the classification task, we use cross-entropy as the measurement of performance.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5) \n",
        "\n",
        "# Initialize trackers, these are not parameters and should not be changed\n",
        "stale = 0\n",
        "best_acc = 0"
      ],
      "metadata": {
        "id": "abcTlCqvWWku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # ---------- Training ----------\n",
        "    # Make sure the model is in train mode before training.\n",
        "    model.train()\n",
        "\n",
        "    # These are used to record information in training.\n",
        "    train_loss = []\n",
        "    train_accs = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "    # ---------- Training ----------\n",
        "    # Make sure the model is in train mode before training.\n",
        "      model.train()\n",
        "\n",
        "    # These are used to record information in training.\n",
        "      train_loss = []\n",
        "      train_accs = []\n",
        "\n",
        "      for batch in tqdm(train_loader):\n",
        "        # A batch consists of image data and corresponding labels.\n",
        "        imgs, labels = batch\n",
        "        print(imgs.shape,labels.shape)\n",
        "\n",
        "        # Forward the data. (Make sure data and model are on the same device.)\n",
        "        logits = model(imgs.to(device))\n",
        "\n",
        "        # Calculate the cross-entropy loss.\n",
        "        # We don't need to apply softmax before computing cross-entropy as it is done automatically. \n",
        "        loss = criterion(logits, labels.to(device))\n",
        "\n",
        "        # Gradients stored in the parameters in the previous step should be cleared out first.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Clip the gradient norms for stable training.\n",
        "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "\n",
        "        # Update the parameters with computed gradients.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute the accuracy for current batch.\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "\n",
        "        # Record the loss and accuracy.\n",
        "        train_loss.append(loss.item())\n",
        "        train_accs.append(acc)\n",
        "        \n",
        "    train_loss = sum(train_loss) / len(train_loss)\n",
        "    train_acc = sum(train_accs) / len(train_accs)\n",
        "\n",
        "    # Print the information.\n",
        "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
        "\n",
        "    # ---------- Validation ----------\n",
        "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
        "    model.eval()\n",
        "\n",
        "    # These are used to record information in validation.\n",
        "    valid_loss = []\n",
        "    valid_accs = []\n",
        "\n",
        "    # Iterate the validation set by batches.\n",
        "    for batch in tqdm(valid_loader):\n",
        "\n",
        "        # A batch consists of image data and corresponding labels.\n",
        "        imgs, labels = batch\n",
        "        #imgs = imgs.half()\n",
        "\n",
        "        # We don't need gradient in validation.\n",
        "        # Using torch.no_grad() accelerates the forward process.\n",
        "        with torch.no_grad():\n",
        "            logits = model(imgs.to(device))\n",
        "\n",
        "        # We can still compute the loss (but not the gradient).\n",
        "        loss = criterion(logits, labels.to(device))\n",
        "\n",
        "        # Compute the accuracy for current batch.\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "\n",
        "        # Record the loss and accuracy.\n",
        "        valid_loss.append(loss.item())\n",
        "        valid_accs.append(acc)\n",
        "        #break\n",
        "\n",
        "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
        "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
        "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
        "\n",
        "    # Print the information.\n",
        "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "\n",
        "    # update logs\n",
        "    if valid_acc > best_acc:\n",
        "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n",
        "    else:\n",
        "        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n",
        "            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
        "\n",
        "\n",
        "    # save models\n",
        "    if valid_acc > best_acc:\n",
        "        print(f\"Best model found at epoch {epoch}, saving model\")\n",
        "        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n",
        "        best_acc = valid_acc\n",
        "        stale = 0\n",
        "    else:\n",
        "        stale += 1\n",
        "        if stale > patience:\n",
        "            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e694c7f483ed465b87fddd3bdc58dd18",
            "b9cce6f969c6440cab9cf359ebbbc487",
            "a303e9a80dd447249cb72962d73e902b",
            "b3cf53f7c55a4a478b1f44e929d516b0",
            "f07e62d7b57c48dfa00163d8d6537372",
            "acb33eb3db6c4ec8934d03a3da8f2645",
            "4b1107e316fb47b684fb39904e8ed0b0",
            "8424813cd63d41439b7c41adf3c26ada",
            "2ab1bec4f85b45dd93f6235e437e846f",
            "a095f91596954376b8b4ff62dd05ae05",
            "6f23d197af454cc7b34ff21c5d345e55"
          ]
        },
        "id": "t6MDJ6eEWk74",
        "outputId": "304f1b97-dab4-4730-ed6e-08df1605472a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/104 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e694c7f483ed465b87fddd3bdc58dd18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./myntradataset/images/14368.jpg\n",
            "./myntradataset/images/1539.jpg\n",
            "./myntradataset/images/14139.jpg\n",
            "./myntradataset/images/17511.jpg\n",
            "./myntradataset/images/12931.jpg\n",
            "./myntradataset/images/10502.jpg\n",
            "./myntradataset/images/12833.jpg\n",
            "./myntradataset/images/17614.jpg\n",
            "./myntradataset/images/16823.jpg\n",
            "./myntradataset/images/15106.jpg\n",
            "./myntradataset/images/16227.jpg\n",
            "./myntradataset/images/17230.jpg\n",
            "./myntradataset/images/10426.jpg\n",
            "./myntradataset/images/14981.jpg\n",
            "./myntradataset/images/16117.jpg\n",
            "./myntradataset/images/15237.jpg\n",
            "./myntradataset/images/10430.jpg\n",
            "./myntradataset/images/15284.jpg\n",
            "./myntradataset/images/12726.jpg\n",
            "./myntradataset/images/14644.jpg\n",
            "./myntradataset/images/11821.jpg\n",
            "./myntradataset/images/15222.jpg\n",
            "./myntradataset/images/11381.jpg\n",
            "./myntradataset/images/13181.jpg\n",
            "./myntradataset/images/14043.jpg\n",
            "./myntradataset/images/12277.jpg\n",
            "./myntradataset/images/11719.jpg\n",
            "./myntradataset/images/14450.jpg\n",
            "./myntradataset/images/10825.jpg\n",
            "./myntradataset/images/13644.jpg\n",
            "./myntradataset/images/10248.jpg\n",
            "./myntradataset/images/10080.jpg\n",
            "./myntradataset/images/17533.jpg\n",
            "./myntradataset/images/11354.jpg\n",
            "./myntradataset/images/15539.jpg\n",
            "./myntradataset/images/12808.jpg\n",
            "./myntradataset/images/17815.jpg\n",
            "./myntradataset/images/12208.jpg\n",
            "./myntradataset/images/14079.jpg\n",
            "./myntradataset/images/13384.jpg\n",
            "./myntradataset/images/11327.jpg\n",
            "./myntradataset/images/10578.jpg\n",
            "./myntradataset/images/10398.jpg\n",
            "./myntradataset/images/13025.jpg\n",
            "./myntradataset/images/12504.jpg\n",
            "./myntradataset/images/12922.jpg\n",
            "./myntradataset/images/15946.jpg\n",
            "./myntradataset/images/16460.jpg\n",
            "./myntradataset/images/17082.jpg\n",
            "./myntradataset/images/16642.jpg\n",
            "./myntradataset/images/11136.jpg\n",
            "./myntradataset/images/12576.jpg\n",
            "./myntradataset/images/11842.jpg\n",
            "./myntradataset/images/17546.jpg\n",
            "./myntradataset/images/12715.jpg\n",
            "./myntradataset/images/15545.jpg\n",
            "./myntradataset/images/10413.jpg\n",
            "./myntradataset/images/14053.jpg\n",
            "./myntradataset/images/1165.jpg\n",
            "./myntradataset/images/10351.jpg\n",
            "./myntradataset/images/17699.jpg\n",
            "./myntradataset/images/17816.jpg\n",
            "./myntradataset/images/11514.jpg\n",
            "./myntradataset/images/15232.jpg\n",
            "torch.Size([64, 3, 128, 128]) torch.Size([64])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-a390814c3c09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Calculate the cross-entropy loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# We don't need to apply softmax before computing cross-entropy as it is done automatically.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Gradients stored in the parameters in the previous step should be cleared out first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 14367 is out of bounds."
          ]
        }
      ]
    }
